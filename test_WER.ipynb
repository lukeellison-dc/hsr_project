{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0.dev20220725\n",
      "0.13.0.dev20220725\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00, -3.8697e-12, -9.5148e-12,  ...,  1.1894e-06,\n",
       "           1.4817e-06,  1.8046e-06]]),\n",
       " 32000,\n",
       " {'client_id': '000abb3006b78ea4c1144e55d9d158f05a9db0110160510fef2b006f2c2c8e35f7bb538b04542511834b61503cdda5b0331566a5cf59dc0d375a44afc4d10777',\n",
       "  'path': 'common_voice_en_27710027.mp3',\n",
       "  'sentence': 'Joe Keaton disapproved of films, and Buster also had reservations about the medium.',\n",
       "  'up_votes': '3',\n",
       "  'down_votes': '1',\n",
       "  'age': '',\n",
       "  'gender': '',\n",
       "  'accents': '',\n",
       "  'locale': 'en',\n",
       "  'segment': ''})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(device)\n",
    "\n",
    "cv_dataset = torchaudio.datasets.COMMONVOICE(\n",
    "    root=\"_cv_corpus/en\",\n",
    "    tsv=\"test.tsv\",\n",
    ")\n",
    "\n",
    "cv_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/test_WER.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/test_WER.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/test_WER.ipynb#ch0000001?line=1'>2</a>\u001b[0m sample_rates \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/test_WER.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m data_item \u001b[39min\u001b[39;00m cv_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/test_WER.ipynb#ch0000001?line=4'>5</a>\u001b[0m     sample_rates[data_item[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/test_WER.ipynb#ch0000001?line=6'>7</a>\u001b[0m pprint(sample_rates)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/datasets/commonvoice.py:68\u001b[0m, in \u001b[0;36mCOMMONVOICE.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    ``up_votes``, ``down_votes``, ``age``, ``gender`` and ``accent``.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker[n]\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m load_commonvoice_item(line, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_header, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_folder_audio, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ext_audio)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/datasets/commonvoice.py:22\u001b[0m, in \u001b[0;36mload_commonvoice_item\u001b[0;34m(line, header, path, folder_audio, ext_audio)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(ext_audio):\n\u001b[1;32m     21\u001b[0m     filename \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ext_audio\n\u001b[0;32m---> 22\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(filename)\n\u001b[1;32m     24\u001b[0m dic \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(header, line))\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m waveform, sample_rate, dic\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py:227\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m--> 227\u001b[0m \u001b[39mreturn\u001b[39;00m _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/io/_compat.py:98\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_audio\u001b[39m(\n\u001b[1;32m     90\u001b[0m     src: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     91\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m     97\u001b[0m     s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mtorchaudio\u001b[39m.\u001b[39mffmpeg_StreamReader(src, \u001b[39mformat\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_audio(s, frame_offset, num_frames, convert, channels_first)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/io/_compat.py:79\u001b[0m, in \u001b[0;36m_load_audio\u001b[0;34m(s, frame_offset, num_frames, convert, channels_first)\u001b[0m\n\u001b[1;32m     77\u001b[0m option: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m     78\u001b[0m s\u001b[39m.\u001b[39madd_audio_stream(i, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, _get_load_filter(frame_offset, num_frames, convert), \u001b[39mNone\u001b[39;00m, option)\n\u001b[0;32m---> 79\u001b[0m s\u001b[39m.\u001b[39;49mprocess_all_packets()\n\u001b[1;32m     80\u001b[0m waveform \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mpop_chunks()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m waveform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "sample_rates = {}\n",
    "\n",
    "for data_item in cv_dataset:\n",
    "    sample_rates[data_item[1]] = 0\n",
    "\n",
    "pprint(sample_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n",
      "sample_rate: 16000\n"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "w2v_model = bundle.get_model().to(device)\n",
    "\n",
    "print(w2v_model.__class__)\n",
    "print(f\"sample_rate: {bundle.sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "\n",
    "LM_WEIGHT1 = 3.23\n",
    "LM_WEIGHT2 = 1.0\n",
    "WORD_SCORE = -0.26\n",
    "\n",
    "beam_search_decoder1 = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=files.tokens,\n",
    "    lm=files.lm,\n",
    "    nbest=3,\n",
    "    beam_size=1500,\n",
    "    lm_weight=LM_WEIGHT1,\n",
    "    word_score=WORD_SCORE,\n",
    ")\n",
    "beam_search_decoder2 = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=files.tokens,\n",
    "    lm=files.lm,\n",
    "    nbest=3,\n",
    "    beam_size=1500,\n",
    "    lm_weight=LM_WEIGHT2,\n",
    "    word_score=WORD_SCORE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ffa323873a4d47b5b1514cb0a08d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_wer1: 0.31724393969763076\n",
      "average_wer2: 0.33207283387666553\n",
      "total_wer1: 0.31346207127623243\n",
      "total_wer2: 0.3258977184131341\n"
     ]
    }
   ],
   "source": [
    "from numpy import average\n",
    "import jiwer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.ExpandCommonEnglishContractions(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.ReduceToListOfListOfWords(word_delimiter=\" \")\n",
    "]) \n",
    "\n",
    "accum_wer1 = 0\n",
    "accum_wer2 = 0\n",
    "hypothesies1 = []\n",
    "hypothesies2 = []\n",
    "truths = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for wav, sr, metadata in tqdm(cv_dataset):\n",
    "        ground_truth = metadata[\"sentence\"]\n",
    "        truths.append(ground_truth)\n",
    "\n",
    "        waveform = torchaudio.functional.resample(wav, sr, bundle.sample_rate)\n",
    "        emission, _ = w2v_model(waveform)\n",
    "        hypothesis1 = ' '.join(beam_search_decoder1(emission[0][None, :])[0][0].words)\n",
    "        hypothesis2 = ' '.join(beam_search_decoder2(emission[0][None, :])[0][0].words)\n",
    "        hypothesies1.append(hypothesis1)\n",
    "        hypothesies2.append(hypothesis2)\n",
    "        # print(f\"ground_truth: {ground_truth}\")\n",
    "        # print(f\"hypothesis1: {hypothesis1}\")\n",
    "        # print(f\"hypothesis2: {hypothesis2}\")\n",
    "        # print(\"WER1:\", jiwer.wer(ground_truth, hypothesis1, truth_transform=transformation, hypothesis_transform=transformation))\n",
    "        # print(\"WER2:\", jiwer.wer(ground_truth, hypothesis2, truth_transform=transformation, hypothesis_transform=transformation))\n",
    "        accum_wer1 += jiwer.wer(ground_truth, hypothesis1, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "        accum_wer2 += jiwer.wer(ground_truth, hypothesis2, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "\n",
    "average_wer1 = accum_wer1 / len(cv_dataset)\n",
    "average_wer2 = accum_wer2 / len(cv_dataset)\n",
    "print(f\"average_wer1: {average_wer1}\")\n",
    "print(f\"average_wer2: {average_wer2}\")\n",
    "\n",
    "total_wer1 = jiwer.wer(truths, hypothesies1, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "total_wer2 = jiwer.wer(truths, hypothesies2, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "print(f\"total_wer1: {total_wer1}\")\n",
    "print(f\"total_wer2: {total_wer2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "jiwer.wer([\"Hello,,,   this is a test\", 'past'], [\"hello this is a test\", 'past'], truth_transform=transformation, hypothesis_transform=transformation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7290f501d4b4b442b548a0f8d71ceee2effc5d79228398df96959ebe12767d93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('hsr_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
