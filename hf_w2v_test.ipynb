{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0.dev20220725\n",
      "0.13.0.dev20220725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "# from datasets import load_dataset, load_metric\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/wav2vec2-base-960h cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(model_name, device)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00, -3.8697e-12, -9.5148e-12,  ...,  1.1894e-06,\n",
       "           1.4817e-06,  1.8046e-06]]),\n",
       " 32000,\n",
       " {'client_id': '000abb3006b78ea4c1144e55d9d158f05a9db0110160510fef2b006f2c2c8e35f7bb538b04542511834b61503cdda5b0331566a5cf59dc0d375a44afc4d10777',\n",
       "  'path': 'common_voice_en_27710027.mp3',\n",
       "  'sentence': 'Joe Keaton disapproved of films, and Buster also had reservations about the medium.',\n",
       "  'up_votes': '3',\n",
       "  'down_votes': '1',\n",
       "  'age': '',\n",
       "  'gender': '',\n",
       "  'accents': '',\n",
       "  'locale': 'en',\n",
       "  'segment': ''})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_dataset = torchaudio.datasets.COMMONVOICE(\n",
    "    root=\"_cv_corpus/en\",\n",
    "    tsv=\"test.tsv\",\n",
    ")\n",
    "\n",
    "cv_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f126e1171e8b4217a1f6c3e5317ddd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=12'>13</a>\u001b[0m             len_longest_sentence \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(metadata[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data, len_longest_sentence\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=15'>16</a>\u001b[0m cv_dataset, len_longest_sentence \u001b[39m=\u001b[39m resample(cv_dataset, processor\u001b[39m.\u001b[39;49mfeature_extractor\u001b[39m.\u001b[39;49msampling_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=16'>17</a>\u001b[0m cv_dataset[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlen longest sentence = \u001b[39m\u001b[39m{\u001b[39;00mlen_longest_sentence\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb Cell 4\u001b[0m in \u001b[0;36mresample\u001b[0;34m(ds, new_sr)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=2'>3</a>\u001b[0m len_longest_sentence \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m wav, sr, metadata \u001b[39min\u001b[39;00m tqdm(ds):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=5'>6</a>\u001b[0m     waveform \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mresample(wav, sr, new_sr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=6'>7</a>\u001b[0m     data\u001b[39m.\u001b[39mappend({\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m\"\u001b[39m: waveform,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msample_rate\u001b[39m\u001b[39m\"\u001b[39m: new_sr,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m: metadata[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000003?line=10'>11</a>\u001b[0m     })\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/datasets/commonvoice.py:68\u001b[0m, in \u001b[0;36mCOMMONVOICE.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    ``up_votes``, ``down_votes``, ``age``, ``gender`` and ``accent``.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker[n]\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m load_commonvoice_item(line, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_header, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_folder_audio, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ext_audio)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/datasets/commonvoice.py:22\u001b[0m, in \u001b[0;36mload_commonvoice_item\u001b[0;34m(line, header, path, folder_audio, ext_audio)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(ext_audio):\n\u001b[1;32m     21\u001b[0m     filename \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ext_audio\n\u001b[0;32m---> 22\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(filename)\n\u001b[1;32m     24\u001b[0m dic \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(header, line))\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m waveform, sample_rate, dic\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py:227\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m--> 227\u001b[0m \u001b[39mreturn\u001b[39;00m _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/io/_compat.py:98\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_audio\u001b[39m(\n\u001b[1;32m     90\u001b[0m     src: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     91\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m     97\u001b[0m     s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mtorchaudio\u001b[39m.\u001b[39mffmpeg_StreamReader(src, \u001b[39mformat\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_audio(s, frame_offset, num_frames, convert, channels_first)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/io/_compat.py:79\u001b[0m, in \u001b[0;36m_load_audio\u001b[0;34m(s, frame_offset, num_frames, convert, channels_first)\u001b[0m\n\u001b[1;32m     77\u001b[0m option: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m     78\u001b[0m s\u001b[39m.\u001b[39madd_audio_stream(i, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, _get_load_filter(frame_offset, num_frames, convert), \u001b[39mNone\u001b[39;00m, option)\n\u001b[0;32m---> 79\u001b[0m s\u001b[39m.\u001b[39;49mprocess_all_packets()\n\u001b[1;32m     80\u001b[0m waveform \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mpop_chunks()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m waveform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def resample(ds, new_sr):\n",
    "    len_longest_sentence = 0\n",
    "    data = []\n",
    "    for wav, sr, metadata in tqdm(ds):\n",
    "        waveform = torchaudio.functional.resample(wav, sr, new_sr)\n",
    "        data.append({\n",
    "            \"speech\": waveform,\n",
    "            \"sample_rate\": new_sr,\n",
    "            \"sentence\": metadata[\"sentence\"]\n",
    "        })\n",
    "        if len(metadata[\"sentence\"]) > len_longest_sentence:\n",
    "            len_longest_sentence = len(metadata[\"sentence\"])\n",
    "    return data, len_longest_sentence\n",
    "\n",
    "cv_dataset, len_longest_sentence = resample(cv_dataset, processor.feature_extractor.sampling_rate)\n",
    "cv_dataset[0]\n",
    "print(f\"len longest sentence = {len_longest_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample):\n",
    "    features = processor(\n",
    "        sample[\"speech\"][0], \n",
    "        sampling_rate=processor.feature_extractor.sampling_rate, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits \n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    sample[\"predicted\"] = processor.batch_decode(pred_ids)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4bf229a653411db18a65712e118604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for x in tqdm(cv_dataset):\n",
    "    result.append(predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speech': tensor([[-7.3703e-13, -7.9181e-12, -7.6020e-12,  ...,  7.5047e-07,\n",
      "          8.4476e-07,  1.6297e-06]]), 'sample_rate': 16000, 'sentence': 'Joe Keaton disapproved of films, and Buster also had reservations about the medium.', 'predicted': ['JO KEEPSAN DISAPPROVED OF THONES AND BUSTER ALSO HAD HESERVATIONS ABOUT THE MEDIUM']}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033d2837d0f441d1a8d0639ea6683299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_wer: 0.4095130565965886\n",
      "total_wer: 0.4011559352830882\n"
     ]
    }
   ],
   "source": [
    "from numpy import average\n",
    "import jiwer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.ExpandCommonEnglishContractions(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.ReduceToListOfListOfWords(word_delimiter=\" \")\n",
    "]) \n",
    "\n",
    "accum_wer = 0\n",
    "hypothesies = []\n",
    "truths = []\n",
    "\n",
    "for sample in tqdm(result):\n",
    "        ground_truth = sample[\"sentence\"]\n",
    "        truths.append(ground_truth)\n",
    "\n",
    "        hypothesis = sample[\"predicted\"][0]\n",
    "        hypothesies.append(hypothesis)\n",
    "        accum_wer += jiwer.wer(ground_truth, hypothesis, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "\n",
    "average_wer = accum_wer / len(result)\n",
    "print(f\"average_wer: {average_wer}\")\n",
    "\n",
    "total_wer = jiwer.wer(truths, hypothesies, truth_transform=transformation, hypothesis_transform=transformation)\n",
    "print(f\"total_wer: {total_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "#### CTC Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JO KEEPSAN DISAPPROVED OF THONES AND BUSTER ALSO HAD HESERVATIONS ABOUT THE MEDIUM']\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0, 29,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  4,  0, 26,\n",
      "          0,  0,  5,  0,  0,  5,  0, 23,  0,  0, 12,  0,  0,  0,  7,  0,  9,  0,\n",
      "          0,  4,  4, 14,  0,  0, 10,  0, 12,  0,  0,  0,  0,  7, 23,  0,  0,  0,\n",
      "         23,  0, 13,  0,  0,  0,  8,  8,  0, 25,  0,  0,  5, 14,  0,  4,  4,  0,\n",
      "          0,  8, 20,  0,  4,  4,  4,  0,  0,  0,  6, 11,  0,  0,  0,  0,  8,  0,\n",
      "          0,  0,  9,  5,  0,  0, 12, 12,  4,  4,  0,  0,  0,  0,  7,  9,  0, 14,\n",
      "          0,  4,  4,  4,  0, 24,  0,  0,  0, 16,  0,  0, 12, 12,  0,  6,  0,  0,\n",
      "          0,  5, 13, 13,  0,  4,  4,  0,  0,  7, 15,  0,  0,  0,  0, 12,  0,  0,\n",
      "          8,  0,  4,  4,  0, 11,  0,  0,  7,  0,  0, 14,  4,  4,  4, 11,  0,  0,\n",
      "          5,  0,  0,  0, 12,  0,  0,  5, 13, 13,  0,  0, 25,  0,  0,  0,  0,  7,\n",
      "          0,  0,  0,  0,  6, 10, 10,  0,  8,  9,  9,  0, 12,  0,  4,  4,  0,  0,\n",
      "          7,  0,  0, 24,  0,  0,  8, 16,  0,  0,  6,  4,  4,  6, 11,  0,  5,  4,\n",
      "          4,  0, 17,  0,  0,  0,  5,  0,  0, 14,  0, 10, 10,  0,  0, 16, 17,  0,\n",
      "          4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[[ 15.3959, -29.3820, -29.0679,  ...,  -7.3007,  -7.6007,  -7.7477],\n",
      "         [ 15.6071, -29.6450, -29.3064,  ...,  -7.5565,  -8.0453,  -7.9907],\n",
      "         [ 15.4609, -29.5302, -29.1838,  ...,  -7.4848,  -8.0367,  -7.9465],\n",
      "         ...,\n",
      "         [ 15.5390, -29.6446, -29.3102,  ...,  -7.3300,  -8.1692,  -7.6694],\n",
      "         [ 15.2772, -29.5289, -29.1972,  ...,  -7.4142,  -8.2163,  -7.7481],\n",
      "         [ 15.3116, -29.4507, -29.1291,  ...,  -7.4923,  -8.1912,  -7.8249]]])\n"
     ]
    }
   ],
   "source": [
    "sample = cv_dataset[0]\n",
    "features = processor(\n",
    "    sample[\"speech\"][0], \n",
    "    sampling_rate=processor.feature_extractor.sampling_rate, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(features.input_values).logits \n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "sample[\"predicted\"] = processor.batch_decode(pred_ids)\n",
    "\n",
    "print(sample[\"predicted\"])\n",
    "print(pred_ids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "re_chars_to_remove = re.compile(r\"[^A-Z ']\")\n",
    "\n",
    "with open(\"vocab.json\", \"r\") as fp:\n",
    "    vocab_dict = json.load(fp)\n",
    "\n",
    "def sentence_to_tensor(sentence, vocab, vocab_size, pad_len):\n",
    "    sentence = sentence.upper()\n",
    "    sentence = re_chars_to_remove.sub('', sentence).replace(' ', '|')\n",
    "    t = torch.zeros([pad_len], dtype=torch.int)\n",
    "    for i,x in enumerate(sentence):\n",
    "        t[i] = vocab[x]\n",
    "    return t, len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29,  8,  4, 26,  5,  5, 23, 12,  7,  9,  4, 14, 10, 12,  7, 23, 23, 13,\n",
      "         8, 25,  5, 14,  4,  8, 20,  4,  6, 11,  8,  9,  5, 12,  4,  7,  9, 14,\n",
      "         4, 24, 16, 12,  6,  5, 13,  4,  7, 15, 12,  8,  4, 11,  7, 14,  4, 11,\n",
      "         5, 12,  5, 13, 25,  7,  6, 10,  8,  9, 12,  4,  7, 24,  8, 16,  6,  4,\n",
      "         6, 11,  5,  4, 17,  5, 14, 10, 16, 17,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.int32)\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 314, 32])\n"
     ]
    }
   ],
   "source": [
    "pad_i = 0\n",
    "target_len = 300\n",
    "target_tensor_from_predicted, real_len = sentence_to_tensor(sample[\"predicted\"][0], vocab_dict, len(vocab_dict), target_len)\n",
    "print(target_tensor_from_predicted)\n",
    "print(target_tensor_from_predicted.unsqueeze(0).shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "target_lengths = torch.tensor([real_len])\n",
    "input_lengths = torch.tensor([logits.shape[1]])\n",
    "print(target_lengths.shape)\n",
    "print(input_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-48.4535)\n"
     ]
    }
   ],
   "source": [
    "ctcloss = torch.nn.CTCLoss(blank=pad_i)\n",
    "loss = ctcloss(logits.squeeze().unsqueeze(1), target_tensor_from_predicted.unsqueeze(0), input_lengths, target_lengths)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    target_tensor_from_predicted[i] = torch.randint(28, (1,))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-48.2106)\n"
     ]
    }
   ],
   "source": [
    "loss = ctcloss(logits.squeeze().unsqueeze(1), target_tensor_from_predicted.unsqueeze(0), input_lengths, target_lengths)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100800])\n",
      "torch.Size([1, 1, 100800])\n",
      "torch.Size([1, 300])\n",
      "tensor([[[-7.3703e-13, -7.9181e-12, -7.6020e-12,  ...,  7.5047e-07,\n",
      "           8.4476e-07,  1.6297e-06]]])\n"
     ]
    }
   ],
   "source": [
    "def transform_and_resample(dataset, new_sample_rate, vocab, batch_size=1):\n",
    "    batch_wavs = []\n",
    "    batch_inputs = []\n",
    "    batch_sentences = []\n",
    "    batch_targets = []\n",
    "    target_lengths = []\n",
    "    TEST_BATCH_CAP = 4\n",
    "    count = 0\n",
    "    for wav, sr, metadata in dataset:\n",
    "        if(len(batch_wavs) == batch_size):\n",
    "            yield {\n",
    "                \"wavs\": torch.stack(batch_wavs),\n",
    "                \"inputs\": torch.stack(batch_inputs),\n",
    "                \"sentences\": batch_sentences,\n",
    "                \"targets\": torch.stack(batch_targets),\n",
    "                \"target_lengths\": torch.tensor(target_lengths),\n",
    "            }\n",
    "            count += 1\n",
    "            batch_wavs = []\n",
    "            batch_inputs = []\n",
    "            batch_sentences = []\n",
    "            batch_targets = []\n",
    "            target_lengths = []\n",
    "        if count == TEST_BATCH_CAP: break\n",
    "        \n",
    "        waveform = torchaudio.functional.resample(wav, sr, new_sample_rate)\n",
    "        batch_wavs.append(waveform)\n",
    "        features = processor(\n",
    "            waveform[0], \n",
    "            sampling_rate=processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch_inputs.append(features.input_values)\n",
    "        batch_sentences.append(metadata[\"sentence\"])\n",
    "        target, length = sentence_to_tensor(metadata[\"sentence\"], vocab, len(vocab), pad_len=300)\n",
    "        batch_targets.append(target)\n",
    "        target_lengths.append(length)\n",
    "\n",
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "def reset_dataloaders():\n",
    "    for phase in ['train', 'test']:\n",
    "        dataset = torchaudio.datasets.COMMONVOICE(\n",
    "            root=\"_cv_corpus/en\",\n",
    "            tsv=f\"{phase}.tsv\",\n",
    "        )\n",
    "        dataset_sizes[phase] = len(dataset)\n",
    "        dataloaders[phase] = transform_and_resample(dataset, processor.feature_extractor.sampling_rate, vocab=vocab_dict, batch_size=1)\n",
    "\n",
    "\n",
    "reset_dataloaders()\n",
    "data = next(dataloaders['test'])\n",
    "print(data[\"wavs\"].shape)\n",
    "print(data[\"inputs\"].shape)\n",
    "print(data[\"targets\"].shape)\n",
    "print(data[\"wavs\"])\n",
    "# reset_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_wer = 100.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        reset_dataloaders()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_wer = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for d in dataloaders[phase]:\n",
    "                inputs = d[\"inputs\"].to(device)\n",
    "                targets = d[\"targets\"].to(device)\n",
    "                target_lengths = d[\"target_lengths\"].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    logits = model(inputs[0]).logits\n",
    "                    # print(f'logits = {logits.shape}')\n",
    "                    # print(f'logits = {logits}')\n",
    "                    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                    pred_sentence = processor.batch_decode(pred_ids)\n",
    "                    print(f'sentence = {d[\"sentences\"][0]}')\n",
    "                    print(f'pred_sentence = {pred_sentence}')\n",
    "\n",
    "                    wer = torchaudio.functional.edit_distance(d[\"sentences\"][0], pred_sentence)\n",
    "                    print(f'wer = {wer}')\n",
    "\n",
    "                    input_lengths = torch.tensor([logits.shape[1]]).to(device)\n",
    "                    # print(f'logits = {logits.shape}')\n",
    "                    # print(f'pred_ids = {pred_ids.shape}')\n",
    "                    # print(f'targets = {targets.shape}')\n",
    "                    # print(f'target_lengths = {target_lengths.shape}')\n",
    "                    loss = criterion(logits.squeeze().unsqueeze(1), targets, input_lengths, target_lengths)\n",
    "                    print(f'loss = {loss}')\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_wer += wer\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_wer = running_wer / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} WER: {epoch_wer:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_wer < best_wer:\n",
    "                best_wer = epoch_wer\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_wer:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "sentence = The Kerry cable stations are recognised as World Heritage Communications Sites.\n",
      "pred_sentence = ['HE CARY CABLE SATION ARE RECOGNIED A EL HAR IS IATIONE']\n",
      "wer = 79\n",
      "loss = -38.75562286376953\n",
      "sentence = So too was the sign at Bromley, Kent.\n",
      "pred_sentence = ['E<unk> C </s>E </s> C </s> <unk> <unk> </s> </s> E C  <unk></s> E   <unk>  <unk>   E </s> <unk> C <unk> </s><unk>  </s> E  <unk>']\n",
      "wer = 37\n",
      "loss = -45.74892807006836\n",
      "sentence = Briles had good reason to record Hey Hank.\n",
      "pred_sentence = ['']\n",
      "wer = 42\n",
      "loss = nan\n",
      "sentence = Tornjaks have a clear, self-confident, serious and calm disposition.\n",
      "pred_sentence = ['']\n",
      "wer = 68\n",
      "loss = nan\n",
      "train Loss: nan WER: 0.0002\n",
      "sentence = Joe Keaton disapproved of films, and Buster also had reservations about the medium.\n",
      "pred_sentence = ['']\n",
      "wer = 83\n",
      "loss = nan\n",
      "sentence = She'll be all right.\n",
      "pred_sentence = ['']\n",
      "wer = 20\n",
      "loss = nan\n",
      "sentence = six\n",
      "pred_sentence = ['']\n",
      "wer = 3\n",
      "loss = nan\n",
      "sentence = All's well that ends well.\n",
      "pred_sentence = ['']\n",
      "wer = 26\n",
      "loss = nan\n",
      "test Loss: nan WER: 0.0081\n",
      "Epoch 1/24\n",
      "----------\n",
      "sentence = The Kerry cable stations are recognised as World Heritage Communications Sites.\n",
      "pred_sentence = ['']\n",
      "wer = 79\n",
      "loss = nan\n",
      "sentence = So too was the sign at Bromley, Kent.\n",
      "pred_sentence = ['']\n",
      "wer = 37\n",
      "loss = nan\n",
      "sentence = Briles had good reason to record Hey Hank.\n",
      "pred_sentence = ['']\n",
      "wer = 42\n",
      "loss = nan\n",
      "sentence = Tornjaks have a clear, self-confident, serious and calm disposition.\n",
      "pred_sentence = ['']\n",
      "wer = 68\n",
      "loss = nan\n",
      "train Loss: nan WER: 0.0002\n",
      "sentence = Joe Keaton disapproved of films, and Buster also had reservations about the medium.\n",
      "pred_sentence = ['']\n",
      "wer = 83\n",
      "loss = nan\n",
      "sentence = She'll be all right.\n",
      "pred_sentence = ['']\n",
      "wer = 20\n",
      "loss = nan\n",
      "sentence = six\n",
      "pred_sentence = ['']\n",
      "wer = 3\n",
      "loss = nan\n",
      "sentence = All's well that ends well.\n",
      "pred_sentence = ['']\n",
      "wer = 26\n",
      "loss = nan\n",
      "test Loss: nan WER: 0.0081\n",
      "Epoch 2/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=2'>3</a>\u001b[0m optimizer_ft \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=3'>4</a>\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=5'>6</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model, ctc_loss, optimizer_ft, exp_lr_scheduler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=12'>13</a>\u001b[0m reset_dataloaders()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=14'>15</a>\u001b[0m \u001b[39m# Each epoch has a training and validation phase\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[1;32m/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb Cell 20\u001b[0m in \u001b[0;36mreset_dataloaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_dataloaders\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=42'>43</a>\u001b[0m     \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=43'>44</a>\u001b[0m         dataset \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCOMMONVOICE(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=44'>45</a>\u001b[0m             root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_cv_corpus/en\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=45'>46</a>\u001b[0m             tsv\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mphase\u001b[39m}\u001b[39;49;00m\u001b[39m.tsv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=46'>47</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=47'>48</a>\u001b[0m         dataset_sizes[phase] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/LEllison/coding/hsr_project/hf_w2v_test.ipynb#ch0000011?line=48'>49</a>\u001b[0m         dataloaders[phase] \u001b[39m=\u001b[39m transform_and_resample(dataset, processor\u001b[39m.\u001b[39mfeature_extractor\u001b[39m.\u001b[39msampling_rate, vocab\u001b[39m=\u001b[39mvocab_dict, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/site-packages/torchaudio/datasets/commonvoice.py:54\u001b[0m, in \u001b[0;36mCOMMONVOICE.__init__\u001b[0;34m(self, root, tsv)\u001b[0m\n\u001b[1;32m     52\u001b[0m walker \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(tsv_, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_header \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(walker)\n\u001b[0;32m---> 54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(walker)\n",
      "File \u001b[0;32m~/manual_install/miniconda3/envs/hsr_project/lib/python3.9/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reset_dataloaders()\n",
    "ctc_loss = torch.nn.CTCLoss()\n",
    "optimizer_ft = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model, ctc_loss, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hsr_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7290f501d4b4b442b548a0f8d71ceee2effc5d79228398df96959ebe12767d93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
